# Comprendre et Construire un Graphe avec LangGraph
Dans ce guide, nous allons découvrir comment construire un graphe avec LangGraph, une bibliothèque complémentaire à LangChain qui permet de créer des flux complexes d'agents IA. Après avoir écrit les chaînes (chains) qui s'exécuteront dans les nœuds, nous allons maintenant nous concentrer sur la construction du graphe lui-même.

# Understanding and Building a Graph with LangGraph
*In this guide, we will discover how to build a graph with LangGraph, a complementary library to LangChain that allows creating complex AI agent workflows. After writing the chains that will run in the nodes, we will now focus on building the graph itself.*

---

# Introduction à LangGraph
Notre objectif est de définir les nœuds du graphe, établir les connexions entre eux (y compris une connexion conditionnelle), compiler le graphe et enfin le visualiser. Nous allons implémenter ce graphe dans un fichier `main.py`.

# Introduction to LangGraph
*Our goal is to define the graph nodes, establish connections between them (including a conditional connection), compile the graph, and finally visualize it. We will implement this graph in a `main.py` file.*

---

# Les importations nécessaires
Commençons par importer les bibliothèques et composants dont nous aurons besoin :

```python
from typing import List, Sequence

from dotenv import load_dotenv
load_dotenv()

from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph import END, MessageGraph

from chains import generate_chain, reflect_chain
```

Nous importons :
- `List` et `Sequence` de la bibliothèque typing
- `BaseMessage` et `HumanMessage` de LangChain
- `END` et `MessageGraph` de LangGraph
- Nos chaînes personnalisées : `generate_chain` et `reflect_chain`

À noter que `END` est une constante qui indique le nœud de terminaison par défaut du graphe. Lorsque nous atteignons ce nœud, l'exécution du graphe s'arrête.

# Required Imports
*Let's start by importing the libraries and components we'll need:*

```python
from typing import List, Sequence

from dotenv import load_dotenv
load_dotenv()

from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph import END, MessageGraph

from chains import generate_chain, reflect_chain
```

*We import:*
- *`List` and `Sequence` from the typing library*
- *`BaseMessage` and `HumanMessage` from LangChain*
- *`END` and `MessageGraph` from LangGraph*
- *Our custom chains: `generate_chain` and `reflect_chain`*

*Note that `END` is a constant that indicates the default termination node of the graph. When we reach this node, the graph execution stops.*

---

# Comprendre le MessageGraph
Le `MessageGraph` est un type de graphe dont l'état est simplement une séquence de messages. Dans notre graphe, chaque nœud recevra en entrée une liste de messages. C'est l'état du graphe, et ce sera l'entrée pour chaque nœud. La sortie de chaque nœud sera un ou plusieurs messages.

Cette implémentation provient directement du code source de LangGraph. Dans sa fonction d'initialisation, nous avons une méthode `add_messages` qui prend la sortie et l'ajoute simplement aux messages de l'état, créant ainsi le nouvel état.

# Understanding MessageGraph
*The `MessageGraph` is a type of graph whose state is simply a sequence of messages. In our graph, each node will receive a list of messages as input. This is the graph's state, and it will be the input for each node. The output of each node will be one or more messages.*

*This implementation comes directly from the LangGraph source code. In its initialization function, we have an `add_messages` method that takes the output and simply adds it to the state messages, thus creating the new state.*

---

# Définition des constantes pour les nœuds
Définissons maintenant deux constantes qui serviront de clés pour nos nœuds :

```python
REFLECT = "reflect"
GENERATE = "generate"
```

Ces constantes représentent les clés de nos nœuds dans le graphe LangGraph :
- Le nœud `REFLECT` exécutera la chaîne de réflexion (reflect_chain)
- Le nœud `GENERATE` exécutera la chaîne de génération (generate_chain)

Ce sont les seuls nœuds personnalisés dont nous aurons besoin. Le graphe utilisera également les nœuds de début et de fin par défaut.

# Defining Constants for Nodes
*Let's now define two constants that will serve as keys for our nodes:*

```python
REFLECT = "reflect"
GENERATE = "generate"
```

*These constants represent the keys of our nodes in the LangGraph graph:*
- *The `REFLECT` node will execute the reflection chain (reflect_chain)*
- *The `GENERATE` node will execute the generation chain (generate_chain)*

*These are the only custom nodes we'll need. The graph will also use the default start and end nodes.*

---

# Implémentation des fonctions de nœuds

## Le nœud de génération
Définissons formellement ce qui se passera dans le nœud de génération :

```python
def generation_node(state: Sequence[BaseMessage]):
    return generate_chain.invoke({"messages": state})
```

Ce nœud reçoit en entrée l'état actuel (une séquence de messages) et exécute notre chaîne de génération. Qu'il s'agisse de la première, deuxième ou troisième exécution, nous allons constamment réviser notre tweet.

En effet, dans notre prompt système, nous avons indiqué que l'IA est un influenceur Twitter qui doit écrire des tweets et les modifier en fonction des réflexions précédentes. Le nœud prend donc le tweet et le révise en fonction de l'état, qui contient toutes les critiques et générations précédentes.

# Implementing Node Functions

## The Generation Node
*Let's formally define what will happen in the generation node:*

```python
def generation_node(state: Sequence[BaseMessage]):
    return generate_chain.invoke({"messages": state})
```

*This node receives the current state (a sequence of messages) as input and executes our generation chain. Whether it's the first, second, or third execution, we will constantly revise our tweet.*

*Indeed, in our system prompt, we indicated that the AI is a Twitter influencer who must write tweets and modify them based on previous reflections. The node therefore takes the tweet and revises it based on the state, which contains all previous critiques and generations.*

---

## Le nœud de réflexion
Implémentons maintenant le nœud de réflexion :

```python
def reflection_node(messages: Sequence[BaseMessage]) -> List[BaseMessage]:
    res = reflect_chain.invoke({"messages": messages})
    return [HumanMessage(content=res.content)]
```

Ce nœud reçoit également une séquence de messages et invoque la chaîne de réflexion. Cependant, il y a une différence subtile mais cruciale : la réponse que nous obtenons du modèle de langage, qui aurait normalement le rôle d'assistant, est transformée en message humain.

Nous prenons le contenu de ce message et le reformatons avec le rôle d'un humain avant de le renvoyer. Pourquoi faire cela ? Nous voulons "tromper" le modèle de langage pour qu'il pense qu'un humain envoie ce message. Ainsi, nous créons une conversation avec le modèle : critique, génération, critique, génération, etc.

En attribuant à la critique le rôle d'un humain, c'est comme si vous aviez une conversation avec ChatGPT pour réviser et améliorer votre tweet. C'est une technique très importante lors de l'implémentation avec LangGraph.

## The Reflection Node
*Let's now implement the reflection node:*

```python
def reflection_node(messages: Sequence[BaseMessage]) -> List[BaseMessage]:
    res = reflect_chain.invoke({"messages": messages})
    return [HumanMessage(content=res.content)]
```

*This node also receives a sequence of messages and invokes the reflection chain. However, there is a subtle but crucial difference: the response we get from the language model, which would normally have the role of assistant, is transformed into a human message.*

*We take the content of this message and reformat it with the role of a human before sending it back. Why do this? We want to "trick" the language model into thinking that a human is sending this message. Thus, we create a conversation with the model: critique, generation, critique, generation, etc.*

*By attributing the critique to the role of a human, it's as if you were having a conversation with ChatGPT to revise and improve your tweet. This is a very important technique when implementing with LangGraph.*

---

# Initialisation et construction du graphe
Initialisons maintenant notre graphe :

```python
builder = MessageGraph()
builder.add_node(GENERATE, generation_node)
builder.add_node(REFLECT, reflection_node)
builder.set_entry_point(GENERATE)
```

Nous créons un objet `MessageGraph`, ajoutons nos deux nœuds avec leurs fonctions respectives, et définissons le nœud `GENERATE` comme point d'entrée.

# Initialization and Building the Graph
*Let's now initialize our graph:*

```python
builder = MessageGraph()
builder.add_node(GENERATE, generation_node)
builder.add_node(REFLECT, reflection_node)
builder.set_entry_point(GENERATE)
```

*We create a `MessageGraph` object, add our two nodes with their respective functions, and define the `GENERATE` node as the entry point.*

---

# Implémentation de la logique conditionnelle
Après avoir exécuté le nœud de génération et révisé le tweet une fois, nous devons décider si le tweet est satisfaisant et si nous pouvons terminer l'exécution du graphe, ou si nous avons besoin d'une étape de réflexion supplémentaire.

Implémentons une fonction qui déterminera la prochaine étape :

```python
def should_continue(state: List[BaseMessage]):
    if len(state) > 6:
        return END
    return REFLECT
```

Cette fonction reçoit l'état (tous nos messages) et décide vers quel nœud nous devons aller ensuite. Elle renvoie soit la clé du nœud de fin (END) si nous avons terminé, soit la clé du nœud de réflexion (REFLECT).

Dans LangGraph, ce type de fonction est appelé "conditional edge" (arête conditionnelle). C'est une fonction qui décide vers quel nœud nous devons aller ensuite.

Ce qui est remarquable avec LangGraph, c'est que cette arête conditionnelle peut théoriquement (et en pratique) utiliser un modèle de langage pour décider si nous voulons terminer ou aller vers un autre nœud. Un modèle de langage peut ainsi raisonner sur la prochaine étape à prendre et le prochain nœud à exécuter, ce qui offre une grande flexibilité.

Dans cet exemple simple, nous n'utilisons pas de modèle de langage pour cette décision, mais une logique qui compte simplement le nombre d'étapes effectuées. Si nous avons dépassé six étapes, nous terminons l'exécution ; sinon, nous passons au nœud de réflexion.

# Implementing Conditional Logic
*After executing the generation node and revising the tweet once, we need to decide if the tweet is satisfactory and if we can finish executing the graph, or if we need an additional reflection step.*

*Let's implement a function that will determine the next step:*

```python
def should_continue(state: List[BaseMessage]):
    if len(state) > 6:
        return END
    return REFLECT
```

*This function receives the state (all our messages) and decides which node we should go to next. It returns either the key of the end node (END) if we have finished, or the key of the reflection node (REFLECT).*

*In LangGraph, this type of function is called a "conditional edge". It's a function that decides which node we should go to next.*

*What is remarkable about LangGraph is that this conditional edge can theoretically (and in practice) use a language model to decide whether we want to finish or go to another node. A language model can thus reason about the next step to take and the next node to execute, which offers great flexibility.*

*In this simple example, we don't use a language model for this decision, but a logic that simply counts the number of steps performed. If we have exceeded six steps, we terminate the execution; otherwise, we move on to the reflection node.*

---

# Configuration des arêtes du graphe
Maintenant, configurons les connexions entre nos nœuds :

```python
builder.add_conditional_edges(GENERATE, should_continue)
builder.add_edge(REFLECT, GENERATE)
```

Nous indiquons à LangGraph qu'après avoir exécuté le nœud `GENERATE` et généré le tweet, nous voulons ajouter une arête conditionnelle. La fonction `should_continue` déterminera si nous devons aller vers le nœud `END` (qui arrêtera l'exécution) ou vers le nœud `REFLECT` (qui utilisera un modèle de langage pour nous donner un retour sur le tweet actuel).

Après avoir obtenu ce retour, nous voulons réviser le tweet, nous ajoutons donc une arête entre le nœud `REFLECT` et le nœud `GENERATE`.

# Configuring Graph Edges
*Now, let's configure the connections between our nodes:*

```python
builder.add_conditional_edges(GENERATE, should_continue)
builder.add_edge(REFLECT, GENERATE)
```

*We tell LangGraph that after executing the `GENERATE` node and generating the tweet, we want to add a conditional edge. The `should_continue` function will determine whether we should go to the `END` node (which will stop execution) or to the `REFLECT` node (which will use a language model to give us feedback on the current tweet).*

*After getting this feedback, we want to revise the tweet, so we add an edge between the `REFLECT` node and the `GENERATE` node.*

---

# Compilation et visualisation du graphe
Nous avons construit tous les nœuds et arêtes nécessaires pour notre graphe. Il est maintenant temps de le compiler :

```python
graph = builder.compile()
```

Cela nous donne l'objet graphe final que nous pouvons utiliser et invoquer.

Pour visualiser notre graphe (ce qui est très utile pour l'expliquer à quelqu'un d'autre ou pour déboguer), nous pouvons utiliser :

```python
print(graph.get_graph().draw_mermaid())
graph.get_graph().print_ascii()
```

La fonction `draw_mermaid()` génère une syntaxe spéciale que nous pouvons copier et coller dans "Mermaid Live" pour obtenir une représentation visuelle. La fonction `print_ascii()` affiche le graphe directement dans la console en ASCII.

# Compiling and Visualizing the Graph
*We have built all the necessary nodes and edges for our graph. It's now time to compile it:*

```python
graph = builder.compile()
```

*This gives us the final graph object that we can use and invoke.*

*To visualize our graph (which is very useful for explaining it to someone else or for debugging), we can use:*

```python
print(graph.get_graph().draw_mermaid())
graph.get_graph().print_ascii()
```

*The `draw_mermaid()` function generates special syntax that we can copy and paste into "Mermaid Live" to get a visual representation. The `print_ascii()` function displays the graph directly in the console in ASCII.*

---

# Exécution du graphe
Enfin, pour exécuter notre graphe avec un tweet initial :

```python
if __name__ == "__main__":
    print("Hello LangGraph")
    inputs = HumanMessage(content="""Améliore ce tweet : :"
                                    @LangChainAI
La nouvelle fonctionnalité "Tool Calling" est vraiment super !

Après une longue attente, elle est enfin là, rendant l'implémentation d'agents à travers différents modèles avec appel de fonctions incroyablement facile. 

J'ai réalisé une vidéo à propos de votre dernier article de blog. 

                                  """)
    response = graph.invoke(inputs)
```

# Executing the Graph
*Finally, to execute our graph with an initial tweet:*

```python
if __name__ == "__main__":
    print("Hello LangGraph")
    inputs = HumanMessage(content="""Make this tweet better:"
                                    @LangChainAI
            — newly Tool Calling feature is seriously underrated.

            After a long wait, it's  here- making the implementation of agents across different models with function calling - super easy.

            Made a video covering their newest blog post

                                  """)
    response = graph.invoke(inputs)
```

---

# Conclusion
Dans ce guide, nous avons appris à construire un graphe avec LangGraph. Nous avons défini des nœuds pour générer et réfléchir, établi des connexions entre eux, y compris une connexion conditionnelle, et visualisé notre graphe.

Cette approche nous permet de créer des flux complexes où les modèles de langage peuvent non seulement générer du contenu, mais aussi réfléchir sur ce contenu et l'améliorer de manière itérative, simulant ainsi un dialogue interne.

LangGraph offre une grande flexibilité dans la construction d'agents IA avancés, permettant même aux modèles de langage de prendre des décisions sur le flux d'exécution lui-même.

# Conclusion
*In this guide, we learned how to build a graph with LangGraph. We defined nodes for generating and reflecting, established connections between them, including a conditional connection, and visualized our graph.*

*This approach allows us to create complex workflows where language models can not only generate content, but also reflect on this content and improve it iteratively, thus simulating an internal dialogue.*

*LangGraph offers great flexibility in building advanced AI agents, even allowing language models to make decisions about the execution flow itself.*

---

# Lexique / Glossary

| Français | English |
|----------|---------|
| graphe | graph |
| nœud | node |
| arête | edge |
| arête conditionnelle | conditional edge |
| chaîne | chain |
| point d'entrée | entry point |
| état | state |
| compiler | compile |
| visualiser | visualize |
| modèle de langage | language model |
| réflexion | reflection |
| génération | generation |